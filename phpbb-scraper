#!/bin/bash
# by Dominik Stanis≈Çaw Suchora <suchora.dominik7@gmail.com>
# License: GNU GPLv3

shopt -s extglob

declare maxprocs='1' url _cookies

IFS=$'\n'

ucurl() {
    curl -k -L -g -m 120 -s -b "$_cookies" --user-agent 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) QtWebEngine/5.15.2 Chrome/87.0.4280.144 Safari/537.36' -H 'Accept-Encoding: gzip, deflate' --compressed "$@"
}

get_topic() {
    [[ "$1" =~ ^http[s]?://([[:alnum:]-]+\.)+[[:alpha:]]+/(.*/)?viewtopic\.php(.*)[\&\?]t=([[:digit:]]+) ]] || { echo "improper url - $1"; return; }
    local -r id="${BASH_REMATCH[4]}"
    [ -e "$id" ] && return
    echo "$1" >&2
    local t="$(ucurl "$1" | tr -d '\t\n\r\a\v')" t2
    local next path

    {
    reliq '
        .title div #page-body; B>h[1-6] [0]; a href=b>"./viewtopic.php" | "%i",
        .path.a { ul #nav-breadcrumbs; a; span itemprop | "%i\n", div #page-header; li .icon-home; a | "%i\n" }
    ' <<< "$t"
    while :
    do
        reliq '
            .posts div #page-body; div #B>"p[0-9]*"; {
                .postid.u div #B>"p[0-9]*" l@[0] | "%(id)v" / sed "s/^p//",
                .date p .author | "%i" / sed "s/<\/time>$//;s/.*>//; s/.*;//; s/^ *//; s/^([a-z]* )+//" "E",
                .content div .content | "%i",
                .signature div .signature #B>sig[0-9]* | "%i",
                .avatar dl .postprofile #B>profile[0-9]*; dt l@[1]; img src | "%(src)v",
                .user dl .postprofile #B>profile[0-9]*; dt l@[1]; a c@[0] | "%i",
                .userid dl .postprofile #B>profile[0-9]*; dt l@[1]; a href c@[0] | "%(href)v" / sed "s/.*[&;]u=([0-9]+).*/\1/" "E",
                .userinfo.a dl .postprofile #B>profile[0-9]*; dd l@[1] m@vf>"&nbsp;" | "%i\n" / sed "s/<strong>([^<]*)<\/strong>/\1/g; s/ +:/:/; /<ul [^>]*class=\"profile-icons\">/{s/.*<a href=\"([^\"]*)\" title=\"Site [^\"]*\".*/Site\t\1/;t;d}; /^[^<>]+:/!{s/^/Rank:/};s/: */\t/" "E"
            } |
        ' <<< "$t"

        t2="$(reliq 'a href rel=next | "%(href)v\n" / sed "s/^\.\///;s/&amp;/\&/g;q"' <<< "$t")"
        [ -z "$t2" ] && t2="$(reliq 'div .topic-actions; div .pagination l@[1]; span l@[1]; E>(a|strong) -href=# | "%(href)v\n" / sed "/^$/{N;s/\n//;s/&amp;/\&/g;s/^\.\///;p;q}" "n"' <<< "$t")"
        next="$url/$t2"
        grep -q '&start=[0-9]*$' <<< "$next" || break
        echo "$next" >&2
        t="$(ucurl "$next" | tr -d '\t\n\r\a\v')"
    done
    } | jq -srcM --arg 'link' "$1" --arg 'id' "$id" '
        {"link":$link,"id":$id}+.[0]+{
            "posts":(.[1:] | map(
                .posts[] | .userinfo |= map(. | split("\t") | {
                    "key":.[0],
                    "value":.[1]
                }
            )
        ))}' > "$id"
}

#get_topic_1() {
#    #[[ "$1" =~ ^http[s]?://([[:alnum:]-]+\.)+[[:alpha:]]+/(.*/)?viewtopic\.php(.*)[\&\?]t=([[:digit:]]+) ]] || { echo "improper url - $1"; return; }
#    #local -r id="${BASH_REMATCH[4]}"
#    #[ -e "$id" ] && return
#    #echo "$1" >&2
#    local t="$(ucurl "$1" | tr -d '\t\n\r\a\v')" t2
#    local next path
#
#    {
#    echo "$1" #link
#    echo "$id" #id
#    echo "$(reliq '* .maintitle; a href=E>"(viewtopic\.php|topic[-_]).*" | "%i\n",
#        * #pageheader; h[1-6]; * c@[0] | "%i\n",
#        div #page-body; h[1-6] @l[1]; a href=b>"./viewtopic\.php" | "%i\n"' <<< "$t" | sed 's/<[^>]*>//g;q')" #title
#    echo "$(reliq 'td -.catHead; * .nav m@E>"(->|&raquo;)"; a -href=b>"#" l@[1] c@[0] | "%i\t",
#        {* .breadcrumbs, * .bc-header} ; a -href=b>"#" l@[1] | "%i\t"' <<< "$t" | awk -v RS='\t' '{ if (i[$0]) {exit} else {i[$0]=1;printf("%s\t",$0)}}')" #path
#
#    #reliq 'table .forumline; tr l@[1] m@B>"<td class=B>\"row[0-9]\""' <<< "$t" | sed 'N;s/\n/ /' 1 2 4 10
#
#
#
#    for i in $(reliq 'table .forumline; tr l@[1] m@B>"<td class=\"row[0-9]\""' <<< "$t" | sed 'N;s/\n/ /')
#    do
#        reliq 'a name=B>"[0-9]*" | "%(name)v\n"' <<< "$i" #postid
#        reliq 'span .postdetails m@v>"<a" | "%i\n"' <<< "$i" | sed 's/^[^:]*: //;s/<.*//' #date
#        t2="$(reliq 'span .postbody | "%i\n"' <<< "$i")"
#        head -n 1 <<< "$t2" #content
#        tail -n 1 <<< "$t2" #signature
#        reliq 'span .gensmall' <<< "$i" #note
#        reliq 'span .postdetails; img src | "%(src)v\n"' <<< "$i" #avatar
#        reliq 'span .name; E>(b|strong) | "%i\n"' <<< "$i" #user
#        reliq 'span .postdetails; a href=E>".*(&amp;|&)u=[0-9]*.*" | "%(href)v\n"' <<< "$i" | sed -E 's/.*(&amp;|&)u=([0-9]*).*/\2/;q' #userid
#        reliq 'span .postdetails m@"<a" | "%i\n"' <<< "$i" | sed -E 's/^ *<br \/>//;s/<br \/> *$//;s/(<br \/>)+/\t/g;s/(^|\t)<[^\t]*\t//; s/<!--[^>]*-->//g; s/(^|\t)([^\t:]+(\t|$))/\1Rank: \2/;s/(^|\t)([^:]+): ([^\t]+)/\1\2^G\3/g' #userinfo
#    done
#
#    reliq 'div #pagecontent; table l@[1]; tr .B>"row[0-9]" l@[1]' | sed 'N;N;s/\n/ /g'
#    reliq 'E>(span|div) .postbody' <<< "$i"
# #not implemented yet}

get_forum() {
    local t t2 next
    echo "$1" >&2
    t="$(ucurl "$1" | tr -d '\n\t\r')"
    while :
    do
        for i in $(reliq 'li; a .forumtitle href | "%(href)v\n" / sed "s/^\.\///;s/&amp;/\&/g"' <<< "$t")
        do
            get_forum "$url/$i"
        done
        for i in $(reliq 'li; a .topictitle href | "%(href)v\n" / sed "s/^\.\///;s/&amp;/\&/g"' <<< "$t")
        do
            [ "$(jobs | wc -l)" -gt "$maxprocs" ] && wait %%
            get_topic "$url/$i" &
        done
        t2="$(reliq 'a href rel=next | "%(href)v\n" / sed "s/^\.\///;s/&amp;/\&/g;q"' <<< "$t")"
        [ -z "$t2" ] && t2="$(reliq 'div .topic-actions; div .pagination l@[1]; span l@[1]; E>(a|strong) -href=# | "%(href)v\n" / sed "/^$/{N;s/\n//;s/&amp;/\&/g;s/^\.\///;p;q}" "n"' <<< "$t")"
        next="$url/$t2"
        grep -q '&start=[0-9]*$' <<< "$next" || break
        echo "$next" >&2
        t="$(ucurl "$next" | tr -d '\t\r\a')";
    done
    wait
}


baseurl() {
    local base="$(dirname "$1")"
    grep -Eq '^http(s)?://([a-zA-Z0-9-]+\.)+[a-zA-Z]+/?$' <<< "$1" && base="$(sed 's#/$##' <<< "$1")"
    echo "$base"
}

usage() {
    printf '%s [OPTION]... [URL]...\n' "${0##*/}"
    printf 'Download, convert to json and save phpbb forums from URL.\n'
    printf 'The 1.x versions of phpbb are not supported yet.\n\n'
    printf 'Options:\n  -d,\t--dir DIR\t\tchange directory to DIR\n'
    printf '  -p,\t--max-procs NUM\t\tset number of processes to run at a time\n'
    printf '  -b,\t--cookie DATA|FILENAME\tpass cookie to curl\n'
    printf '  -t,\t--topic URL\t\tpass URL as topic link\n'
    printf '  -f,\t--forum URL\t\tpass URL as forum link\n'
    printf '  -h,\t--help\t\t\tshow this message\n'
}

[ "$#" -eq 0 ] && { usage >&2; exit 1; }

while [ "$#" -gt 0 ]
do
    case "$1" in
        -d|--dir) cd "$2" || break; shift;;
        -t|--topic)
            url="$(baseurl "$2")"
            get_topic "$2"
            shift;;
        -f|--forum)
            url="$(baseurl "$2")"
            get_forum "$2"
            shift;;
        -b|--cookie) _cookies="$2"; shift;;
        -p|--max-procs) maxprocs="$2"; shift;;
        -h|--help) usage >&2; exit;;
        -*) usage >&2; exit 1;;
        *) break;;
    esac
    shift
done

while [ "$#" -gt 0 ]
do
    url="$(baseurl "$1")"
    case "$1" in
        http?(s)://+(+([[:alnum:]-]).)+([[:alpha:]])/?(*/)viewtopic.php*[\&\?]t=+([[:digit:]])*)
            get_topic "$1";;
        http?(s)://+(+([[:alnum:]-]).)+([[:alpha:]])/?(*/)viewforum.php*)
            get_forum "$1";;
        http?(s)://+(+([[:alnum:]-]).)+([[:alpha:]])/?(*/)index.php*)
            get_forum "$1";;
        http?(s)://+(+([[:alnum:]-]).)+([[:alpha:]])*)
            get_forum "$1";
    esac
    shift
done
